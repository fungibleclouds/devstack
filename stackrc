# stackrc - Configuration for DevStack

# Common configuration defaults

# Destination path for installation ``DEST``
DEST=/opt/stack

# Normally ``TOP_DIR`` is set before calling stackrc; this is
# a last-chance value, and is the only value in this file that
# is set to be overridden
TOP_DIR=${TOP_DIR:-$DEST/devstack}

# ``FILES`` contains config templates and other useful files such as the
# list of **apt** and **pip** dependencies
FILES=$TOP_DIR/files

# Timeouts
# --------

# Service startup timeout
SERVICE_TIMEOUT=60

# Specify which services to launch.  These generally correspond to screen tabs
ENABLED_SERVICES=g-api,g-reg,key,n-api,n-crt,n-obj,n-cpu,n-net,n-sch,n-novnc,n-xvnc,n-cauth,horizon,mysql,rabbit

# compute service
NOVA_REPO=https://github.com/openstack/nova.git
NOVA_BRANCH=master

# storage service
# ---------------

SWIFT_REPO=https://github.com/openstack/swift.git
SWIFT_BRANCH=master
SWIFT_DIR=$DEST/swift

# By default the location of swift drives and objects is located inside
# the swift source directory. SWIFT_DATA_LOCATION variable allow you to redefine
# this.
SWIFT_DATA_LOCATION=${SWIFT_DIR}/data

# We are going to have the configuration files inside the source
# directory, change SWIFT_CONFIG_LOCATION if you want to adjust that.
SWIFT_CONFIG_LOCATION=${SWIFT_CONFIG_LOCATION:-${SWIFT_DIR}/config}

# devstack will create a loop-back disk formatted as XFS to store the
# swift data. By default the disk size is 1 gigabyte. The variable
# SWIFT_LOOPBACK_DISK_SIZE specified in bytes allow you to change
# that.
SWIFT_LOOPBACK_DISK_SIZE=1000000

# The ring uses a configurable number of bits from a pathâ€™s MD5 hash as
# a partition index that designates a device. The number of bits kept
# from the hash is known as the partition power, and 2 to the partition
# power indicates the partition count. Partitioning the full MD5 hash
# ring allows other parts of the cluster to work in batches of items at
# once which ends up either more efficient or at least less complex than
# working with each item separately or the entire cluster all at once.
# By default we define 9 for the partition count (which mean 512).
SWIFT_PARTITION_POWER_SIZE=9

# swift and keystone integration
SWIFT_KEYSTONE_REPO=https://github.com/cloudbuilders/swift-keystone2.git
SWIFT_KEYSTONE_BRANCH=master
SWIFT_KEYSTONE_DIR=$DEST/swift-keystone2

# glance: image catalog service
# -----------------------------

GLANCE_REPO=https://github.com/openstack/glance.git
GLANCE_BRANCH=master
GLANCE_DIR=$DEST/glance


# keystone: identity management
# -----------------------------

KEYSTONE_REPO=https://github.com/openstack/keystone.git
KEYSTONE_BRANCH=master

# Set default Keystone interface configuration
# set hosts to "" to use ``SERVICE_HOST``
KEYSTONE_AUTH_HOST=""
KEYSTONE_AUTH_PORT=35357
KEYSTONE_AUTH_PROTOCOL=http
KEYSTONE_SERVICE_HOST=""
KEYSTONE_SERVICE_PORT=5000
KEYSTONE_SERVICE_PROTOCOL=http


# a websockets/html5 or flash powered VNC console for vm instances
NOVNC_REPO=https://github.com/cloudbuilders/noVNC.git
NOVNC_BRANCH=master

# django powered web control panel for openstack
HORIZON_REPO=https://github.com/openstack/horizon.git
HORIZON_BRANCH=master

# python client library to nova that horizon (and others) use
NOVACLIENT_REPO=https://github.com/openstack/python-novaclient.git
NOVACLIENT_BRANCH=master

# python keystone client library to nova that horizon uses
KEYSTONECLIENT_REPO=https://github.com/openstack/python-keystoneclient
KEYSTONECLIENT_BRANCH=master

# quantum service
QUANTUM_REPO=https://github.com/openstack/quantum
QUANTUM_BRANCH=master

# quantum client
QUANTUM_CLIENT_REPO=https://github.com/openstack/python-quantumclient
QUANTUM_CLIENT_BRANCH=master

# Tempest test suite
TEMPEST_REPO=https://github.com/openstack/tempest.git
TEMPEST_BRANCH=master

# melange service
MELANGE_REPO=https://github.com/openstack/melange.git
MELANGE_BRANCH=master

# python melange client library
MELANGECLIENT_REPO=https://github.com/openstack/python-melangeclient.git
MELANGECLIENT_BRANCH=master

# Specify a comma-separated list of uec images to download and install into glance.
# supported urls here are:
#  * "uec-style" images:
#     If the file ends in .tar.gz, uncompress the tarball and and select the first
#     .img file inside it as the image.  If present, use "*-vmlinuz*" as the kernel
#     and "*-initrd*" as the ramdisk
#     example: http://cloud-images.ubuntu.com/releases/oneiric/release/ubuntu-11.10-server-cloudimg-amd64.tar.gz
#  * disk image (*.img,*.img.gz)
#    if file ends in .img, then it will be uploaded and registered as a to
#    glance as a disk image.  If it ends in .gz, it is uncompressed first.
#    example:
#      http://cloud-images.ubuntu.com/releases/oneiric/release/ubuntu-11.10-server-cloudimg-armel-disk1.img
#      http://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-rootfs.img.gz
#IMAGE_URLS="http://smoser.brickies.net/ubuntu/ttylinux-uec/ttylinux-uec-amd64-11.2_2.6.35-15_1.tar.gz" # old ttylinux-uec image
#IMAGE_URLS="http://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img" # cirros full disk image
case "$LIBVIRT_TYPE" in
    lxc) # the cirros root disk in the uec tarball is empty, so it will not work for lxc
        IMAGE_URLS="http://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-rootfs.img.gz";;
    *)  # otherwise, use the uec style image (with kernel, ramdisk, disk)
        IMAGE_URLS="http://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-uec.tar.gz";;
esac

# allow local overrides of env variables
if [ -f $TOP_DIR/localrc ]; then
    source $TOP_DIR/localrc
fi
